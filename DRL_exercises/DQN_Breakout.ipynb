{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c851b77",
   "metadata": {},
   "source": [
    "# Play Breakout using Deep Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f801d869",
   "metadata": {},
   "source": [
    "As an agent takes actions and moves through an environment, it learns to map the observed state of the environment to an action. An agent will choose an action in a given state based on a **Q-value**, which is a weighted reward based on the expected highest long-term reward. \n",
    "\n",
    "A **Q-Learning Agent** learns to perform its task such that the recommended action **maximizes the potential future rewards**. This method is considered an *Off-Policy* method, meaning its Q values are updated assuming that the best action was chosen, even if the best action was not chosen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b9b317",
   "metadata": {},
   "source": [
    "## Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8326a309",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as ks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb021224",
   "metadata": {},
   "source": [
    "## Define Buffer class for Experience Replay"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cfc487d",
   "metadata": {},
   "source": [
    "In this class we only have to define the buffer dimension that will limit the number of samples contained by the buffer. This limits the amount of memory required by the program and avoi problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a06e80ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Buffer:\n",
    "    \n",
    "    def __init__(self, buffer_dim):\n",
    "        \n",
    "        self.buffer_dim = buffer_dim\n",
    "        self.state_hist = []\n",
    "        self.action_hist = []\n",
    "        self.rewards_hist = []\n",
    "        self.next_state_hist = []\n",
    "        self.done_hist = []\n",
    "    \n",
    "    # Save a sample in the Buffer\n",
    "    def save(self, state, action, reward, next_state, done):\n",
    "        self.state_hist.append(state)\n",
    "        self.action_hist.append(action)\n",
    "        self.rewards_hist.append(reward)\n",
    "        self.next_state_hist.append(next_state)\n",
    "        self.done_hist.append(done)\n",
    "        \n",
    "        # Deleting the oldest sample\n",
    "        if len(self.done_hist) > self.buffer_dim:\n",
    "            del self.state_hist[0]\n",
    "            del self.action_hist[0]\n",
    "            del self.rewards_hist[0]\n",
    "            del self.next_state_hist[0]\n",
    "            del self.done_hist[0]\n",
    "    \n",
    "    # Get a batch of samples from the Buffer\n",
    "    def sample(self, batch_size):\n",
    "        indices = np.random.choice(range(len(self.done_hist)), size=batch_size)\n",
    "        \n",
    "        state_sample = np.array([self.state_hist[i] for i in indices])\n",
    "        action_sample = [self.action_hist[i] for i in indices]\n",
    "        reward_sample = [self.rewards_hist[i] for i in indices]\n",
    "        next_state_sample = np.array([self.next_state_hist[i] for i in indices])\n",
    "        done_sample = tf.convert_to_tensor([float(self.done_hist[i]) for i in indices])\n",
    "        \n",
    "        return state_sample, action_sample, reward_sample, next_state_sample, done_sample\n",
    "    \n",
    "    # Get the number of samples contained in the Buffer\n",
    "    def n_samples(self):\n",
    "        return len(self.done_hist)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52351783",
   "metadata": {},
   "source": [
    "## Define Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48751045",
   "metadata": {},
   "source": [
    "This network **learns an approximation of the Q-table**, which is a mapping between the states and actions that an agent will take. For every state we'll have four actions, that can be taken. The environment provides the state, and the action is chosen by selecting the larger of the four Q-values predicted in the output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a38e0fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(input_shape, num_actions):\n",
    "    \n",
    "    inputs = ks.layers.Input(input_shape)\n",
    "    \n",
    "    x = ks.layers.Conv2D(32, kernel_size=8, strides=4, activation='relu')(inputs)\n",
    "    x = ks.layers.Conv2D(64, kernel_size=4, strides=2, activation='relu')(x)\n",
    "    x = ks.layers.Conv2D(64, kernel_size=3, strides=1, activation='relu')(x)\n",
    "    \n",
    "    x = ks.layers.Flatten()(x)\n",
    "    x = ks.layers.Dense(512, activation='relu')(x)\n",
    "    outputs = ks.layers.Dense(num_actions, activation='linear')(x)\n",
    "\n",
    "    return ks.Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50eeb4f5",
   "metadata": {},
   "source": [
    "## Create the Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "428c4c7b",
   "metadata": {},
   "source": [
    "In this environment, a board moves along the bottom of the screen returning a ball that will destroy blocks at the top of the screen. The aim of the game is to remove all blocks and breakout of the level. The agent must learn to control the board by moving left and right, returning the ball and removing all the blocks without the ball passing the board.\n",
    "\n",
    "We will **stack 4 frames** together in a way that the Convolutional Neural Network has the opportunity to 'perceive' the movement of the ball. We are using the **Grayscale images** instead of the RGB ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1223539",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14, 1335387034)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make('ALE/Breakout-v5')\n",
    "\n",
    "# Transforming the environment\n",
    "env = gym.wrappers.AtariPreprocessing(env, frame_skip=1)\n",
    "\n",
    "# Stacking frames together\n",
    "env = gym.wrappers.FrameStack(env, 4)\n",
    "env.seed(14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cadcd165",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PARAMETERS\n",
    "episodes = 1000\n",
    "\n",
    "# Discount factor\n",
    "gamma = 0.99\n",
    "\n",
    "# Exploration trade-off\n",
    "initial_exploration_frames = 100000\n",
    "epsilon = 1.0\n",
    "epsilon_min = 0.05\n",
    "epsilon_max = 1.0\n",
    "decay_interval = 1000000\n",
    "\n",
    "batch_size = 32\n",
    "max_steps_per_episode = 10000\n",
    "\n",
    "# Train the model after 4 actions\n",
    "update_after_actions = 4\n",
    "\n",
    "# Update target network after 10000 actions\n",
    "update_target_network = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9f5c3203",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the models\n",
    "model = get_model((84, 84, 4), env.action_space.n)\n",
    "model_target = get_model((84, 84, 4), env.action_space.n)\n",
    "\n",
    "# Choice of optimizer and loss function\n",
    "optimizer = ks.optimizers.Adam(learning_rate=0.00025, clipnorm=1.0)\n",
    "loss_function = ks.losses.Huber()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "946c6417",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 84, 84)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d72a9f4d",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af6d306d",
   "metadata": {},
   "source": [
    "The DQN algorithm can be describes as follows:\n",
    "\n",
    "1. **Initialize replay buffer**,\n",
    "\n",
    "2. Pre-process and the environment and **feed state S to DQN**, which will return the Q values of all possible actions in the state.\n",
    "\n",
    "3. **Select an action** using the epsilon-greedy policy: with the probability epsilon, we select a random action A and with probability 1-epsilon. Select an action that has a maximum Q value, such as A = argmax(Q(S, A, θ)).\n",
    "\n",
    "4. After selecting the action A, the Agent **performs chosen action** in a state S and move to a new state S’ and receive a reward R.\n",
    "\n",
    "5. **Store transition** in replay buffer as <S,A,R,S’>.\n",
    "\n",
    "6. Next, **sample some random batches of transitions** from the replay buffer and calculate the loss using the formula:\n",
    "\n",
    "7. **Perform gradient descent** with respect to actual network parameters in order to minimize this loss.\n",
    "\n",
    "8. After every k steps, **copy our actual network weights to the target network weights**.\n",
    "\n",
    "9. Repeat these steps for M number of episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423857fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer = Buffer(100000)\n",
    "\n",
    "episode_reward_hist = []\n",
    "running_reward = 0\n",
    "frames = 0\n",
    "for episode in range(1, episodes+1):\n",
    "    \n",
    "    # Reset environment\n",
    "    state = env.reset()\n",
    "    state = np.array(state)\n",
    "    state = np.transpose(state, [1, 2, 0])\n",
    "    done = False\n",
    "    episode_reward = 0\n",
    "    steps = 0\n",
    "    \n",
    "    # Starting the episode\n",
    "    while not done and steps < max_steps_per_episode:\n",
    "        \n",
    "        env.render()\n",
    "        \n",
    "        frames += 1\n",
    "        steps += 1\n",
    "        \n",
    "        # Epsilon-greedy strategy\n",
    "        if frames < initial_exploration_frames or np.random.rand() < epsilon:\n",
    "            # Random action\n",
    "            action = np.random.choice(env.action_space.n)\n",
    "        else:\n",
    "            # Get probabilities from the model\n",
    "            state_tensor = tf.convert_to_tensor(state)\n",
    "            state_tensor = tf.expand_dims(state_tensor, 0)\n",
    "            action_probs = model(state_tensor, training=False)\n",
    "            \n",
    "            # Get best action\n",
    "            action = tf.argmax(action_probs[0]).numpy()\n",
    "            \n",
    "        # Compute action\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        next_state = np.array(next_state)\n",
    "        next_state = np.transpose(next_state, [1, 2, 0])\n",
    "        episode_reward += reward\n",
    "        \n",
    "        # Save in Replay Buffer\n",
    "        buffer.save(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        \n",
    "        # Update epsilon\n",
    "        epsilon -= (epsilon_max - epsilon_min) / decay_interval\n",
    "        epsilon = max(epsilon, epsilon_min)\n",
    "        \n",
    "        # Update conditions\n",
    "        if buffer.n_samples() > batch_size and frames % update_after_actions == 0:\n",
    "            \n",
    "            # Load samples\n",
    "            state_sample, action_sample, reward_sample, next_state_sample, done_sample = buffer.sample(batch_size)\n",
    "            \n",
    "            # Estimate future rewards using the target network\n",
    "            estimated_rewards = model_target.predict(next_state_sample)\n",
    "            \n",
    "            # Q-value estimate = current reward + estimated future reward\n",
    "            estimated_q = reward_sample + gamma * tf.reduce_max(estimated_rewards, axis=1) \n",
    "            \n",
    "            # If final frame set the last value to -1\n",
    "            estimated_q = estimated_q * (1 - done_sample) - done_sample\n",
    "            \n",
    "            # Create a mask so we only calculate loss on the updated Q-values\n",
    "            masks = tf.one_hot(action_sample, env.action_space.n)\n",
    "            \n",
    "            with tf.GradientTape() as tape:\n",
    "                \n",
    "                # Current Q-values\n",
    "                q_values = model(state_sample)\n",
    "                q_action = tf.reduce_sum(tf.multiply(q_values, masks), axis=1)\n",
    "                \n",
    "                # Evaluate Loss\n",
    "                loss = loss_function(estimated_q, q_action)\n",
    "                \n",
    "            # Backpropagation\n",
    "            grads = tape.gradient(loss, model.trainable_variables)\n",
    "            optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "            \n",
    "        if frames % update_target_network == 0:\n",
    "            \n",
    "            # Update Terget network\n",
    "            model_target.set_weights(model.get_weights())\n",
    "            \n",
    "            print(f'Running reward {running_reward:.2f} at frame {frames} (episode {episode})')\n",
    "            \n",
    "    episode_reward_hist.append(episode_reward)\n",
    "    if len(episode_reward_hist) > 100:\n",
    "        del episode_reward_hist[0]\n",
    "        \n",
    "    running_reward = np.mean(episode_reward_hist)\n",
    "    \n",
    "    if running_reward > 40:\n",
    "        print(f'Solved at episode {episode}')\n",
    "        break\n",
    "        \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe046f2",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a44dee15",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_episodes = 10\n",
    "reward_hist = []\n",
    "\n",
    "for test_episode in range(1, test_episodes+1):\n",
    "    \n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    episode_reward = 0\n",
    "    \n",
    "    while not done:\n",
    "        env.render()\n",
    "        \n",
    "        state = np.array(state)\n",
    "        state = np.transpose(state, [1, 2, 0])\n",
    "        state_tensor = tf.convert_to_tensor(state)\n",
    "        state_tensor = tf.expand_dims(state_tensor, 0)\n",
    "        \n",
    "        action_probs = model(state_tensor)\n",
    "        action = tf.argmax(action_probs[0]).numpy()\n",
    "        \n",
    "        state, reward, done, _ = env.step(action)\n",
    "        episode_reward += reward\n",
    "        \n",
    "    reward_hist.append(episode_reward)\n",
    "    \n",
    "env.close()\n",
    "\n",
    "print(f'Average episode reward: {np.mean(reward_hist)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
