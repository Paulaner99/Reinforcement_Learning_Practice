{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab14ca50",
   "metadata": {},
   "source": [
    "## Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e5bfc44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as ks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7acd54fe",
   "metadata": {},
   "source": [
    "## Create Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "326b46a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"LunarLander-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81f0ecef",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_actions = env.action_space.n\n",
    "num_inputs = env.observation_space.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ddf643",
   "metadata": {},
   "source": [
    "## Define the REINFORCE Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b8855c",
   "metadata": {},
   "source": [
    "A simple implementation of this algorithm would involve creating a Policy: a model that takes a state as input and **generates the probability of taking an action** as output. \n",
    "\n",
    "A policy guides the agent telling it what action to take at each state. The policy is then iterated on and tweaked slightly at each step until we get a policy that solves the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "074ab6b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reinforce(input_shape, num_actions):\n",
    "    \n",
    "    inputs = ks.layers.Input(input_shape)\n",
    "    \n",
    "    x = ks.layers.Dense(128, activation='relu')(inputs)\n",
    "    x = ks.layers.Dense(128, activation='relu')(x)\n",
    "    \n",
    "    outputs = ks.layers.Dense(num_actions, 'softmax')(x)\n",
    "    \n",
    "    return ks.Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d97582d",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "915760cf",
   "metadata": {},
   "source": [
    "The steps involved in the implementation of REINFORCE would be as follows:\n",
    "\n",
    "1. Initialize a **Random Policy** (a NN that takes the state as input and returns the probability of actions).\n",
    "2. Use the policy to **play N steps of the game** â€” record action probabilities (from policy), reward (from environment), action (from agent).\n",
    "3. **Calculate the discounted reward** for each step by backpropagation.\n",
    "4. **Calculate expected reward** G.\n",
    "5. **Adjust weights** of Policy (back-propagate error in NN) to increase G.\n",
    "6. Repeat from 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "85d76863",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PARAMETERS\n",
    "episodes = 1000\n",
    "max_steps_per_episode = 10000\n",
    "\n",
    "gamma = 0.99\n",
    "alpha = 1e-2\n",
    "\n",
    "load_model = False\n",
    "load_path = os.path.join('Saved_Models', 'LunarLander_REINFORCE_1')\n",
    "\n",
    "save_model = False\n",
    "save_path = os.path.join('Saved_Models', 'LunarLander_REINFORCE_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7131a20b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running reward: -76.33529553681956 (epsiode 10)\n",
      "Running reward: -108.12131583540261 (epsiode 20)\n",
      "Running reward: -202.22965426387276 (epsiode 30)\n",
      "Running reward: -191.29892265866852 (epsiode 40)\n",
      "Running reward: -139.1466327814471 (epsiode 50)\n",
      "Running reward: -143.58864288225996 (epsiode 60)\n",
      "Running reward: -102.92698041496942 (epsiode 70)\n",
      "Running reward: -130.395686159895 (epsiode 80)\n",
      "Running reward: -111.8149292348624 (epsiode 90)\n",
      "Running reward: -91.78967488444806 (epsiode 100)\n",
      "Running reward: -78.82487885655614 (epsiode 110)\n",
      "Running reward: -62.65056542430481 (epsiode 120)\n",
      "Running reward: -73.659972655785 (epsiode 130)\n",
      "Running reward: -78.51299557056288 (epsiode 140)\n",
      "Running reward: -50.01548222856215 (epsiode 150)\n",
      "Running reward: -70.06813464236625 (epsiode 160)\n",
      "Running reward: -60.9790433276316 (epsiode 170)\n",
      "Running reward: -57.47701732203816 (epsiode 180)\n",
      "Running reward: -47.02098232408225 (epsiode 190)\n",
      "Running reward: -56.50535369990282 (epsiode 200)\n",
      "Running reward: -76.0641793267285 (epsiode 210)\n",
      "Running reward: -93.18996582373526 (epsiode 220)\n",
      "Running reward: -73.0475405636781 (epsiode 230)\n",
      "Running reward: -63.2687680542196 (epsiode 240)\n",
      "Running reward: -25.948746915689917 (epsiode 250)\n",
      "Running reward: -15.969821943359321 (epsiode 260)\n",
      "Running reward: -18.4780391568602 (epsiode 270)\n",
      "Running reward: -37.167113545892896 (epsiode 280)\n",
      "Running reward: -50.829888208379714 (epsiode 290)\n",
      "Running reward: -32.86239940357163 (epsiode 300)\n",
      "Running reward: -10.450381610022355 (epsiode 310)\n",
      "Running reward: -29.53139271645008 (epsiode 320)\n",
      "Running reward: -54.06138038100447 (epsiode 330)\n",
      "Running reward: -68.23477469977468 (epsiode 340)\n",
      "Running reward: -52.36748862808639 (epsiode 350)\n",
      "Running reward: -70.34994544494056 (epsiode 360)\n",
      "Running reward: -54.77283305930115 (epsiode 370)\n",
      "Running reward: -72.50133783846518 (epsiode 380)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create the model\n",
    "model = get_reinforce(num_inputs, num_actions)\n",
    "\n",
    "# Optimizer selection\n",
    "optimizer = ks.optimizers.Adam(learning_rate=alpha)\n",
    "\n",
    "if load_model is True:\n",
    "    model.load_weights(load_path)\n",
    "\n",
    "action_prob_hist = []\n",
    "action_hist = []\n",
    "reward_hist = []\n",
    "\n",
    "running_reward = 0\n",
    "episode = 1\n",
    "\n",
    "while True:\n",
    "    \n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    \n",
    "    episode_reward = 0\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        # EPISODE GENERATION\n",
    "        for step in range(1, max_steps_per_episode):\n",
    "            \n",
    "            #env.render()\n",
    "            \n",
    "            state = tf.convert_to_tensor(state)\n",
    "            state = tf.expand_dims(state, 0)\n",
    "\n",
    "            action_probs = model(state)\n",
    "            action = np.random.choice(num_actions, p=np.squeeze(action_probs))\n",
    "\n",
    "            state, reward, done, _ = env.step(action)\n",
    "\n",
    "            episode_reward += reward\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "            action_prob_hist.append(tf.math.log(action_probs[0, action]))\n",
    "            action_hist.append(action)\n",
    "            reward_hist.append(reward)  \n",
    "        \n",
    "        running_reward = 0.05 * episode_reward + (1 - 0.05) * running_reward\n",
    "\n",
    "        returns = []\n",
    "        G = 0\n",
    "        for r in reward_hist[::-1]:\n",
    "            G = r + gamma * G \n",
    "            returns.insert(0, G)\n",
    "\n",
    "        # Normalization\n",
    "        returns = np.array(returns)\n",
    "        returns = (returns - np.mean(returns)) / np.std(returns)\n",
    "        returns = returns.tolist()\n",
    "\n",
    "        # UPDATE POLICY\n",
    "        history = zip(action_prob_hist, reward_hist)\n",
    "        losses = []\n",
    "        for log_prob, rew in history:\n",
    "            losses.append(-log_prob * rew)\n",
    "\n",
    "        loss_value = sum(losses)\n",
    "        grads = tape.gradient(loss_value, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "        # Clear the loss and reward history\n",
    "        action_prob_hist.clear()\n",
    "        action_hist.clear()\n",
    "        reward_hist.clear()\n",
    "\n",
    "        if episode % 10 == 0:\n",
    "            print(f'Running reward: {running_reward} (epsiode {episode})')\n",
    "            \n",
    "            if save_model is True:\n",
    "                model.save_weights(save_path)\n",
    "        \n",
    "        if running_reward > 195:  # Condition to consider the task solved\n",
    "            print(f\"Solved at episode {episode}!\")\n",
    "            \n",
    "            if save_model is True:\n",
    "                model.save_weights(save_path)\n",
    "                \n",
    "            break\n",
    "            \n",
    "        episode += 1\n",
    "        \n",
    "#env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "34158fe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for episode in range(10):\n",
    "    \n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    \n",
    "    for step in range(1, max_steps_per_episode):\n",
    "\n",
    "        env.render()    \n",
    "        \n",
    "        state = tf.convert_to_tensor(state)\n",
    "        state = tf.expand_dims(state, 0)\n",
    "\n",
    "        action_probs = model(state)\n",
    "        action = np.random.choice(num_actions, p=np.squeeze(action_probs))\n",
    "\n",
    "        state, reward, done, _ = env.step(action)\n",
    "\n",
    "        if done:\n",
    "            break       \n",
    "            \n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
