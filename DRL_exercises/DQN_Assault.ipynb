{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3040fc8",
   "metadata": {},
   "source": [
    "## Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c526df34",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as ks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9084e46",
   "metadata": {},
   "source": [
    "## Create the Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b9ee63",
   "metadata": {},
   "source": [
    "You control a vehicle that can move sideways. A big mother ship circles overhead and continually deploys smaller drones. You must destroy these enemies and dodge their attacks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0756c45e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "env = gym.make(\"ALE/Assault-v5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ffdc3ae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Atari preprocessing wrapper\n",
    "env = gym.wrappers.AtariPreprocessing(env, \n",
    "                                      noop_max=30, \n",
    "                                      frame_skip=1, \n",
    "                                      terminal_on_life_loss=False, \n",
    "                                      grayscale_obs=True, \n",
    "                                      grayscale_newaxis=False, \n",
    "                                      scale_obs=False)\n",
    "\n",
    "#Frame stacking\n",
    "env = gym.wrappers.FrameStack(env, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48bafb6f",
   "metadata": {},
   "source": [
    "## Define Buffer class for Experience Replay"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4eb3811",
   "metadata": {},
   "source": [
    "In this class we only have to define the buffer dimension that will limit the number of samples contained by the buffer. This limits the amount of memory required by the program and avoi problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3a9b4afd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class Buffer:\n",
    "    def __init__(self, buffer_dim, batch_size=32):\n",
    "        self.state = []\n",
    "        self.action = []\n",
    "        self.reward = []\n",
    "        self.done = []\n",
    "        self.next_state = []\n",
    "        \n",
    "        self.buffer_dim = buffer_dim\n",
    "        self.batch_size = batch_size\n",
    "        self.idx = 0\n",
    "        \n",
    "    def save_sample(self, state, action, reward, done, next_state):\n",
    "        self.state.append(state)\n",
    "        self.action.append(action)\n",
    "        self.reward.append(reward)\n",
    "        self.done.append(done)\n",
    "        self.next_state.append(next_state)\n",
    "        self.idx += 1\n",
    "        \n",
    "        if self.idx >= buffer_dim:\n",
    "            del self.state[:1]\n",
    "            del self.action[:1]\n",
    "            del self.reward[:1]\n",
    "            del self.done[:1]\n",
    "            del self.next_state[:1]\n",
    "        \n",
    "    def get_sample(self):\n",
    "        \n",
    "        last = self.idx % self.buffer_dim\n",
    "        idx = np.random.randint(0, self.idx) % self.buffer_dim\n",
    "        \n",
    "        # We can't take the last 'batch_size' samples\n",
    "        while last-self.batch_size < idx <= last:\n",
    "            idx = np.random.randint(0, self.idx) % self.buffer_dim\n",
    "        \n",
    "        \n",
    "        return (np.array(self.state[idx:idx+self.batch_size]),\n",
    "                self.action[idx:idx+self.batch_size],\n",
    "                self.reward[idx:idx+self.batch_size],\n",
    "                tf.convert_to_tensor([float(self.done[i]) for i in range(idx, idx+self.batch_size)]),\n",
    "                np.array(self.next_state[idx:idx+self.batch_size]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a10f35",
   "metadata": {},
   "source": [
    "## Define Deep Q-Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a2c9170",
   "metadata": {},
   "source": [
    "This network **learns an approximation of the Q-table**, which is a mapping between the states and actions that an agent will take. For every state we'll have four actions, that can be taken. The environment provides the state, and the action is chosen by selecting the larger of the four Q-values predicted in the output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c6361c17",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 84, 84)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c0238000",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_actions = env.action_space.n\n",
    "input_shape = (84, 84, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c7ee63b1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_Q_model(input_shape, num_actions):\n",
    "    inp = ks.layers.Input(input_shape)\n",
    "    \n",
    "    x = ks.layers.Conv2D(32, 8, strides=4, activation='relu', padding='same')(inp)\n",
    "    x = ks.layers.Conv2D(64, 4, strides=2, activation='relu', padding='same')(x)\n",
    "    x = ks.layers.Conv2D(64, 3, strides=1, activation='relu', padding='same')(x)\n",
    "    \n",
    "    x = ks.layers.Flatten()(x)\n",
    "    x = ks.layers.Dense(512, activation='relu')(x)\n",
    "    out = ks.layers.Dense(num_actions, activation='linear')(x)\n",
    "    \n",
    "    model = ks.Model(inp, out)\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = get_Q_model(input_shape, num_actions)\n",
    "model_target = get_Q_model(input_shape, num_actions)\n",
    "\n",
    "# Optimizer and Loss function\n",
    "optimizer = ks.optimizers.Adam(learning_rate=0.00025, clipnorm=1.0)\n",
    "loss_function = ks.losses.Huber()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d8b1aa4",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d61e4a",
   "metadata": {},
   "source": [
    "The DQN algorithm can be describes as follows:\n",
    "\n",
    "1. **Initialize replay buffer**,\n",
    "\n",
    "2. Pre-process and the environment and **feed state S to DQN**, which will return the Q values of all possible actions in the state.\n",
    "\n",
    "3. **Select an action** using the epsilon-greedy policy: with the probability epsilon, we select a random action A and with probability 1-epsilon. Select an action that has a maximum Q value, such as A = argmax(Q(S, A, θ)).\n",
    "\n",
    "4. After selecting the action A, the Agent **performs chosen action** in a state S and move to a new state S’ and receive a reward R.\n",
    "\n",
    "5. **Store transition** in replay buffer as <S,A,R,S’>.\n",
    "\n",
    "6. Next, **sample some random batches of transitions** from the replay buffer and calculate the loss using the formula:\n",
    "\n",
    "7. **Perform gradient descent** with respect to actual network parameters in order to minimize this loss.\n",
    "\n",
    "8. After every k steps, **copy our actual network weights to the target network weights**.\n",
    "\n",
    "9. Repeat these steps for M number of episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7e33a5c2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# PARAMETERS\n",
    "episodes = 1000\n",
    "\n",
    "gamma = 0.99\n",
    "epsilon_start = 1\n",
    "epsilon_end = 0.1\n",
    "decade_period = 1000000\n",
    "\n",
    "update_period = 10000\n",
    "\n",
    "buffer_dim = 1000000\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7156ae07",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "epsilon = epsilon_start\n",
    "\n",
    "buffer = Buffer(buffer_dim, batch_size)\n",
    "\n",
    "episode_reward_history = []\n",
    "average_reward = 0\n",
    "step = 0\n",
    "for episode in range(1, episodes+1):\n",
    "    \n",
    "    state = np.array(env.reset())\n",
    "    state = np.transpose(state, [1, 2, 0])\n",
    "    done = False\n",
    "    episode_reward = 0\n",
    "    \n",
    "    while not done:\n",
    "        \n",
    "        env.render()\n",
    "        \n",
    "        # Choose action\n",
    "        if np.random.uniform() < epsilon:\n",
    "            action = np.random.choice(num_actions)\n",
    "        else:\n",
    "            state_tensor = tf.convert_to_tensor(state)\n",
    "            state_tensor = tf.expand_dims(state_tensor, 0)\n",
    "            action_probs = model(state_tensor, training=False)\n",
    "            \n",
    "            # Take best action\n",
    "            action = tf.argmax(action_probs[0]).numpy()\n",
    "            \n",
    "        # Epsilon decay\n",
    "        rate = np.max((decade_period - step) / decade_period, 0)\n",
    "        epsilon = (epsilon_start - epsilon_end) * rate + epsilon_end\n",
    "        \n",
    "        # Apply the sampled action in our environment\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        next_state = np.array(next_state)\n",
    "        next_state = np.transpose(next_state, [1, 2, 0])\n",
    "        \n",
    "        episode_reward += reward\n",
    "        \n",
    "        # Save actions and states in replay buffer\n",
    "        buffer.save_sample(state, action, reward, done, next_state)\n",
    "        \n",
    "        state = next_state\n",
    "        step += 1\n",
    "        \n",
    "        # After batch_size steps we can start sampling \n",
    "        if step > batch_size:\n",
    "            state_sample, action_sample, reward_sample, done_sample, next_state_sample = buffer.get_sample()\n",
    "            \n",
    "            # Q-values estimates (TARGET MODEL) for the sampled future states\n",
    "            future_rewards = model_target.predict(next_state_sample)\n",
    "            \n",
    "            # Current Q-value estimate\n",
    "            updated_q_values = reward_sample + gamma * tf.reduce_max(future_rewards, axis=1)\n",
    "            \n",
    "            # If final frame set the last value to -1\n",
    "            updated_q_values = updated_q_values * (1 - done_sample) - done_sample\n",
    "            \n",
    "            # Create a mask so we only calculate loss on the updated Q-values\n",
    "            masks = tf.one_hot(action_sample, num_actions)\n",
    "            \n",
    "            with tf.GradientTape() as tape:\n",
    "                # Train the model on the states and updated Q-values\n",
    "                q_values = model(state_sample)\n",
    "                \n",
    "                # Apply the masks to the Q-values to get the Q-value for action taken\n",
    "                q_action = tf.reduce_sum(tf.multiply(q_values, masks), axis=1)\n",
    "                \n",
    "                # Calculate loss between new Q-value and old Q-value\n",
    "                loss = loss_function(updated_q_values, q_action)\n",
    "                \n",
    "            # Backpropagation\n",
    "            grads = tape.gradient(loss, model.trainable_variables)\n",
    "            optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "        \n",
    "        # Update the model\n",
    "        if step % update_period == 0:\n",
    "            \n",
    "            # Set the new weights\n",
    "            model_target.set_weights(model.get_weights())\n",
    "            \n",
    "            print(f\"Average reward: {average_reward:.2f} at episode {episode}\")\n",
    "            \n",
    "    episode_reward_history.append(episode_reward)\n",
    "    if len(episode_reward_history) > 100:\n",
    "        del episode_reward_history[:1]\n",
    "    average_reward = np.mean(episode_reward_history)\n",
    "    \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c0f954",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ab6e1065",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\anaconda3\\lib\\site-packages\\gym\\envs\\atari\\environment.py:267: UserWarning: \u001b[33mWARN: We strongly suggest supplying `render_mode` when constructing your environment, e.g., gym.make(ID, render_mode='human'). Using `render_mode` provides access to proper scaling, audio support, and proper framerates.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average episode reward: 14.7\n"
     ]
    }
   ],
   "source": [
    "reward_hist = []\n",
    "\n",
    "for episode in range(1, episodes+1):\n",
    "    \n",
    "    state = np.array(env.reset())\n",
    "    done = False\n",
    "    episode_reward = 0\n",
    "    \n",
    "    while not done:\n",
    "        \n",
    "        env.render()\n",
    "        \n",
    "        state = np.transpose(state, [1, 2, 0])\n",
    "        state_tensor = tf.convert_to_tensor(state)\n",
    "        state_tensor = tf.expand_dims(state_tensor, 0)\n",
    "        action_probs = model(state_tensor, training=False)\n",
    "            \n",
    "        # Take best action\n",
    "        action = tf.argmax(action_probs[0]).numpy()\n",
    "        \n",
    "        # Apply the sampled action in our environment\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        state = np.array(state)\n",
    "        \n",
    "        episode_reward += reward\n",
    "        \n",
    "    reward_hist.append(episode_reward)\n",
    "    \n",
    "env.close()\n",
    "\n",
    "print(f'Average episode reward: {np.mean(reward_hist)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11859a5a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
